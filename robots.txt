# Allow all web crawlers full access to the website
User-agent: *
Disallow:

# Block crawlers from accessing certain directories (e.g., admin pages, sensitive files)
Disallow: /close/login

# Block all listed bad bots
User-agent: AhrefsBot
Disallow: /

User-agent: SemrushBot
Disallow: /

User-agent: DotBot
Disallow: /

User-agent: MJ12bot
Disallow: /

User-agent: Baiduspider
Disallow: /

User-agent: YandexBot
Disallow: /

User-agent: PetalBot
Disallow: /

User-agent: Sogou Spider
Disallow: /

User-agent: SeznamBot
Disallow: /

User-agent: DataForSeoBot
Disallow: /

# Block generic scraping and command-line tools
User-agent: Python-urllib
Disallow: /

User-agent: Scrapy
Disallow: /

User-agent: curl
Disallow: /

User-agent: wget
Disallow: /

User-agent: HTTrack
Disallow: /

User-agent: EmailCollector
Disallow: /

User-agent: LinkChecker
Disallow: /

# Block bots with empty or missing user-agent headers
User-agent: *
Disallow: /
Crawl-delay: 10

# Link to your XML sitemap for better crawlability
Sitemap: https://www.univault.in/sitemap.xml
